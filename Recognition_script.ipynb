{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0,
     62,
     117,
     173,
     189,
     194,
     206,
     226,
     282,
     310,
     329,
     382,
     456
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Miniconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Miniconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#------------- All the packages and paths\n",
    "import pytesseract\n",
    "import cv2\n",
    "from object_detection.utils import ops as utils_ops\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import peakutils\n",
    "import Levenshtein\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import statistics as stats\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "# A Tesseract executable needs to be imported for the OCR\n",
    "# Tesseract Repo: https://github.com/tesseract-ocr/tessdoc\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = \"C:/tensorflow1/models/research/object_detection/inference_graph/frozen_inference_graph.pb\"\n",
    "\n",
    "path_to_vid = \"C:/tensorflow1/models/research/object_detection/own_testing/test3.mp4\"\n",
    "\n",
    "#-------------- Load a (frozen) Tensorflow model into memory.\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "#------------- Loading label map\n",
    "\n",
    "# Instead of saving it as JSON earlier, I now retyped it manually as a dictionary - not smart, but works.\n",
    "category_index = {1:  {'id': 1,  'name': 'app'}, 2: {'id': 2,  'name': 'activity'}, 3: {'id': 3,  'name': 'not_over'}, 4:\n",
    "                  {'id': 4,  'name': 'not_app'}, 5: {'id': 5,  'name': 'pick_over'}, 6: {'id': 6,  'name': 'pick_app'},\n",
    "                  7: {'id': 7,  'name': 'use_over'}, 8: {'id': 8,  'name': 'week'}}\n",
    "\n",
    "#------------- Object Detection Functions\n",
    "\n",
    "\n",
    "\n",
    "# A function that outputs the boxes and\n",
    "## all the auxiliary data for an image.\n",
    "### Implemented to process all images in one TF session\n",
    "#### to save time initializing a seession for every image\n",
    "def detect_multiple_images(images, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            output_dicts = []\n",
    "            for index, image in enumerate(images):\n",
    "                ops = tf.get_default_graph().get_operations()\n",
    "                all_tensor_names = {\n",
    "                    output.name for op in ops for output in op.outputs}\n",
    "                tensor_dict = {}\n",
    "                for key in ['num_detections', 'detection_boxes', 'detection_scores',\n",
    "                            'detection_classes', 'detection_masks']:\n",
    "                    tensor_name = key + ':0'\n",
    "                    if tensor_name in all_tensor_names:\n",
    "                        tensor_dict[key] = tf.get_default_graph(\n",
    "                        ).get_tensor_by_name(tensor_name)\n",
    "                if 'detection_masks' in tensor_dict:\n",
    "                    # The following processing is only for single image\n",
    "                    detection_boxes = tf.squeeze(\n",
    "                        tensor_dict['detection_boxes'], [0])\n",
    "                    detection_masks = tf.squeeze(\n",
    "                        tensor_dict['detection_masks'], [0])\n",
    "                    # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                    real_num_detection = tf.cast(\n",
    "                        tensor_dict['num_detections'][0], tf.int32)\n",
    "                    detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
    "                                               real_num_detection, -1])\n",
    "                    detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
    "                                               real_num_detection, -1, -1])\n",
    "                    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                        detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                    detection_masks_reframed = tf.cast(\n",
    "                        tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                    # Follow the convention by adding back the batch dimension\n",
    "                    tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                        detection_masks_reframed, 0)\n",
    "                image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "                # Run inference\n",
    "                output_dict = sess.run(tensor_dict,\n",
    "                                       feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "                # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "                output_dict['num_detections'] = int(\n",
    "                    output_dict['num_detections'][0])\n",
    "                output_dict['detection_classes'] = output_dict[\n",
    "                    'detection_classes'][0].astype(np.uint8)\n",
    "                output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "                output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "                if 'detection_masks' in output_dict:\n",
    "                    output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "                output_dicts.append(output_dict)\n",
    "    return output_dicts\n",
    "\n",
    "# getting absolute box coordinates from the relative ones (output of running an inference graph),\n",
    "## only keeping boxes that detected a field with certainty,\n",
    "### returns a list with [(coordinates), probability, label type]\n",
    "def get_abs_coord(image_np, output_dict, thresh):\n",
    "    coordinates = vis_util.return_coordinates(\n",
    "        image_np,\n",
    "        np.squeeze(output_dict['detection_boxes']),\n",
    "        np.squeeze(output_dict['detection_classes']).astype(np.int32),\n",
    "        np.squeeze(output_dict['detection_scores']),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=8,\n",
    "        skip_labels=False,\n",
    "        min_score_thresh=thresh)\n",
    "    return coordinates\n",
    "\n",
    "#------------ Reading the frames\n",
    "\n",
    "# Helper functions for key frame detection\n",
    "def scale(img, xScale, yScale):\n",
    "    res = cv2.resize(img, None, fx=xScale, fy=yScale,\n",
    "                     interpolation=cv2.INTER_AREA)\n",
    "    return res\n",
    "\n",
    "def convert_frame_to_grayscale(frame):\n",
    "    grayframe = None\n",
    "    gray = None\n",
    "    if frame is not None:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = scale(gray, 1, 1)\n",
    "        grayframe = scale(gray, 1, 1)\n",
    "        gray = cv2.GaussianBlur(gray, (9, 9), 0.0)\n",
    "    return grayframe, gray\n",
    "\n",
    "# A function to load a video into numpy arrays by frame, with the possibility to skip n frames.\n",
    "# Below a more sophisticated version is included, too\n",
    "def vid_to_np(path, frame_skip):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    all_vids = []\n",
    "    count = 0\n",
    "    skipper = frame_skip\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if count < (length - skipper):\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            all_vids.append(frame)\n",
    "            count += skipper\n",
    "            cap.set(1, count)\n",
    "        else:\n",
    "            cap.release()\n",
    "            break\n",
    "    return all_vids\n",
    "\n",
    "# Thres stands for threshold - how dissimilar do the frames have to be included?\n",
    "# Possible values range: 0 - 1\n",
    "def keyframeDetection(source, Thres):\n",
    "\n",
    "    # Open the video\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if (cap.isOpened() == False):\n",
    "        print(\"Error opening video file\")\n",
    "\n",
    "    # Empty lists that will be populated\n",
    "    lstfrm = []\n",
    "    lstdiffMag = []\n",
    "    images = []\n",
    "    full_color = []\n",
    "    lastFrame = None\n",
    "\n",
    "    # Read until video is completed\n",
    "    for i in range(1, length):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Grayscale\n",
    "            grayframe, blur_gray = convert_frame_to_grayscale(frame)\n",
    "            # Keeping count of frames to keep comparing to the last one\n",
    "            frame_number = cap.get(cv2.CAP_PROP_POS_FRAMES) - 1\n",
    "            lstfrm.append(frame_number)\n",
    "            images.append(grayframe)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            full_color.append(rgb_frame)\n",
    "            if frame_number == 0:\n",
    "                lastFrame = blur_gray\n",
    "            # Calculate the difference between two frames\n",
    "            diff = cv2.subtract(blur_gray, lastFrame)\n",
    "            diffMag = cv2.countNonZero(diff)\n",
    "            lstdiffMag.append(diffMag)\n",
    "            # Current frame becomes last frame\n",
    "            lastFrame = blur_gray\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    y = np.array(lstdiffMag)\n",
    "    # A neat function to index the frames the difference between which meets the threshold\n",
    "    # Props to Stack Overflow for this\n",
    "    # To like 70% of the whole code, matter of fact\n",
    "    base = peakutils.baseline(y, 2)\n",
    "    indices = peakutils.indexes(y-base, Thres, min_dist=1)\n",
    "\n",
    "    final = []\n",
    "    for x in indices:\n",
    "        final.append(full_color[x])\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    return(final)\n",
    "\n",
    "\n",
    "#---------------- OCR functions\n",
    "\n",
    "# Takes an app image and returns a dataframe with the detected words\n",
    "# Trail determines how much to cut off from the right side\n",
    "def app_to_text(img, leftSide, blurry, trail, v_type):\n",
    "    height, width, channels = img.shape\n",
    "    if ph1 < 120:\n",
    "        img = cv2.bitwise_not(img)\n",
    "    # resizing if the image is too small\n",
    "    img = cv2.resize(img, (int(width*(150/height)), 150),\n",
    "                     interpolation=cv2.INTER_LANCZOS4)\n",
    "    # pre-processing for better OCR\n",
    "    # here, it's cropped to remove the app logo\n",
    "    # also, we can remove the overall time with 'trail' (~150px), or set it to 0 if needed\n",
    "    img = img[0:150, leftSide:(int(width*(150/height))-trail)]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img = cv2.medianBlur(img, blurry)\n",
    "    if v_type == \"weekly\":\n",
    "        img = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "                    cv2.THRESH_BINARY,19,9)\n",
    "        img = cv2.medianBlur(img, 5)\n",
    "    else: \n",
    "        ret, img = cv2.threshold(img, black_point, 255, cv2.THRESH_BINARY)\n",
    "    ## texter = pytesseract.image_to_string(img)\n",
    "    datum = pytesseract.image_to_data(\n",
    "        img, output_type=pytesseract.Output.DATAFRAME)\n",
    "\n",
    "    #img = cv2.medianBlur(img,3)\n",
    "    return datum\n",
    "\n",
    "# A function that takes the image with 'activity' (time)\n",
    "## and transforms that into text (string)\n",
    "def activ_to_text(img):\n",
    "    height, width, channels = img.shape\n",
    "    if ph1 < 120:\n",
    "        img = cv2.bitwise_not(img)\n",
    "    # resizing if the image is too small\n",
    "    img = cv2.resize(img, (int(width*(150/height)), 150),\n",
    "                     interpolation=cv2.INTER_LANCZOS4)\n",
    "    # pre-processing for better OCR\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    img = cv2.medianBlur(img, 3)\n",
    "    ret, img = cv2.threshold(img, black_point, 255, cv2.THRESH_BINARY)\n",
    "    # detecting the text\n",
    "    texter = pytesseract.image_to_string(img)\n",
    "    for rep in replacements:\n",
    "        # replacing the common mistakes\n",
    "        texter = texter.replace(rep[0], rep[1])\n",
    "    return texter\n",
    "\n",
    "# processing notification and phone pick up apps\n",
    "def process_week_app(images):\n",
    "    week_apps = []\n",
    "    for image in images:\n",
    "        s = app_to_text(image,130, 3, 0, video_type)\n",
    "        s = s.dropna()\n",
    "\n",
    "        if len(s) < 2:\n",
    "            continue\n",
    "        \n",
    "        week_app = []\n",
    "        app_name = []\n",
    "        app_name_conf = []\n",
    "        app_time = []\n",
    "\n",
    "        for index, row in s.iterrows():\n",
    "            # for the name, it's the first block and line\n",
    "            if row[\"block_num\"] == 1 and row[\"line_num\"] == 1:\n",
    "                app_name.append(row[\"text\"])\n",
    "                # Also getting OCR confidence for the name\n",
    "                app_name_conf.append(row[\"conf\"])\n",
    "        # If there's nothing, go to the next one        \n",
    "        if len(app_name) == 0:\n",
    "            continue\n",
    "            \n",
    "        app_name_conf = stats.mean(app_name_conf)\n",
    "        # If unsure, go to the next one\n",
    "        if app_name_conf < 90:\n",
    "            continue\n",
    "        if s.iloc[-1, -2] < 60:\n",
    "            continue\n",
    "\n",
    "        app_name = \" \".join(app_name)\n",
    "        # Getting the number from the last row of the df\n",
    "        app_time = s.iloc[-1, -1]\n",
    "        week_app = [app_name, app_time]\n",
    "        if len(week_apps) == 0:\n",
    "            week_apps.append(week_app)\n",
    "        else:\n",
    "            counter = 0\n",
    "            for ii in week_apps:\n",
    "                # check if the app is already in the list\n",
    "                if fuzz.ratio(app_name, ii[0]) > 75:\n",
    "                    counter += 1\n",
    "                    break\n",
    "            if counter == 0:\n",
    "                week_apps.append(week_app)\n",
    "            else:\n",
    "                continue\n",
    "    return week_apps\n",
    "\n",
    "# Takes the dataframe, outputs the app name and the app times\n",
    "# Importanly, also includes recognition confidence for the name\n",
    "# Output: [(name, conf), [time, time2 if exists]]\n",
    "def process_app(app_img):\n",
    "    # We get a pandas dataframe\n",
    "    s = app_to_text(app_img, 140, 5, 150, video_type)\n",
    "    s = s.dropna()\n",
    "\n",
    "    app_name = []\n",
    "    app_name_conf = []\n",
    "    app_time = []\n",
    "    # Iterate over the rows of the df, to get the name and the time\n",
    "    for index, row in s.iterrows():\n",
    "        # for the name, it's the first block and line\n",
    "        if row[\"block_num\"] == 1 and row[\"line_num\"] == 1:\n",
    "            app_name.append(row[\"text\"])\n",
    "            # Also getting OCR confidence for the name\n",
    "            app_name_conf.append(row[\"conf\"])\n",
    "        else:\n",
    "            # Everything else is time by exclusion\n",
    "            # Fixing commong digit recognition mistakes,\n",
    "            # so only taking words that are probably time-indicators\n",
    "            if len(row[\"text\"]) < 4:\n",
    "                temp_app = \"\"\n",
    "                for rep in digit_repl:\n",
    "                    temp_app = row[\"text\"].replace(rep[0], rep[1])\n",
    "                app_time.append(temp_app)\n",
    "            else:\n",
    "                app_time.append(row[\"text\"])\n",
    "    if len(app_name_conf) == 0:\n",
    "        return [(\"\", 0), 0]\n",
    "\n",
    "    app_name_conf = stats.mean(app_name_conf)\n",
    "    app_name = \" \".join(app_name)\n",
    "    app_time = \" \".join(app_time)\n",
    "\n",
    "    for rep in app_replacements:\n",
    "        # Replacing the common mistakes\n",
    "        app_time = app_time.replace(rep[0], rep[1])\n",
    "    # Splitting the on-screen and bg times\n",
    "    app_time = app_time.split(\"-\")\n",
    "    app_tuple = (app_name, app_name_conf)\n",
    "    return [app_tuple, app_time]\n",
    "\n",
    "# Only keeps the unique and healthy apps per list (hour)\n",
    "def apps_this_hour(hour):\n",
    "    this_hour = []\n",
    "    # start counting from the second element, since the first is 'activity'\n",
    "    for i in range(1, len(hour)):\n",
    "        app = process_app(hour[i][1])\n",
    "        # Only consider apps with confidence above 90\n",
    "        if app[0][1] < 90:\n",
    "            continue\n",
    "        # If the name is empty, or if something is wrong with the times, skip\n",
    "        elif len(app[0][0].strip()) == 0 or len(app[1][0]) > 13:\n",
    "            continue\n",
    "        else:\n",
    "            # append the app if it's the first one\n",
    "            if len(this_hour) == 0:\n",
    "                this_hour.append(app)\n",
    "                continue\n",
    "            counter = 1\n",
    "            for ii in this_hour:\n",
    "                # check if the app is already in the list\n",
    "                if fuzz.ratio(app[0][0], ii[0][0]) > 75:\n",
    "                    counter = 0\n",
    "                    break\n",
    "            # if it is not, append it\n",
    "            if counter > 0:\n",
    "                this_hour.append(app)\n",
    "    return this_hour\n",
    "\n",
    "# Using the extracted key frames and the coordinates of the bounding boxes\n",
    "# Format is: first layer - hours\n",
    "# second layer: first element is time, all subsequent - apps\n",
    "# third layer: first element is the label, second - app image\n",
    "# output[i][0][1] - time; output[i][1:ii][1] - app image\n",
    "def photos_to_dicts(photos, coordinates):\n",
    "    sorted_dicts = []\n",
    "    # take each screen and the respective box coordinates\n",
    "    for i, photo in enumerate(photos):\n",
    "        for part in coordinates[i]:\n",
    "            # if the box coordinate is 'activity', create a new list\n",
    "            if part[5] == 2:\n",
    "                img = photo[part[0]:part[1], part[2]:part[3]]\n",
    "                # get the text from the image\n",
    "                texter = activ_to_text(img)\n",
    "                # check if it looks like it should\n",
    "                ##print(texter)\n",
    "                var1 = fuzz.ratio(texter, \"ACTIVITY (00:00-00:00)\")\n",
    "                var2 = fuzz.ratio(texter, \"ACTIVITY BY APP (00:00-00:00)\")\n",
    "                sim_score = max([var1, var2])\n",
    "                if sim_score > 80:\n",
    "                    # append immediately if it's the first time 'activity' is encountered\n",
    "                    if len(sorted_dicts) == 0:\n",
    "                        sorted_dicts.append([[part[5], texter]])\n",
    "                        ##print(\"first time hour shown\")\n",
    "                        continue\n",
    "                    # append a new one if it's a new hour\n",
    "                    if fuzz.ratio(texter, sorted_dicts[-1][0][1]) < 99:\n",
    "                        sorted_dicts.append([[part[5], texter]])\n",
    "                        ##print(\"new hour\")\n",
    "            # if the box coordinate is 'app', append it to the latest 'activity'\n",
    "            if part[5] == 1:\n",
    "                if len(sorted_dicts) > 0:\n",
    "                    img = photo[part[0]:part[1], part[2]:part[3]]\n",
    "                    sorted_dicts[-1].append([part[5], img])\n",
    "    return sorted_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hV4P5gyTWKMI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_dirs = os.listdir(\"D:/SinSu/all_videos\")\n",
    "outputs = {}\n",
    "for index, direct in enumerate(all_dirs):\n",
    "    path_to_vid = \"D:/SinSu/all_videos/\" + direct\n",
    "\n",
    "    #Loading a sample of frames to determine the brightness of the image\n",
    "    check_frames = vid_to_np(path_to_vid,\n",
    "                               60)\n",
    "    ph1 = [np.mean(x) for x in check_frames]\n",
    "    ph1 = stats.mean(ph1)\n",
    "\n",
    "    # 0.17 for dark, 0.28 for light\n",
    "    ## Based on the brightness, make some adjustments\n",
    "    ## Needed since different image proccesing happens for l. vs d. videos\n",
    "    if ph1 < 120:\n",
    "        photos = keyframeDetection(path_to_vid, 0.21)\n",
    "        print(\"frames nr.: {}\".format(len(photos)))\n",
    "        black_point = 215\n",
    "        print(\"dark video\")\n",
    "    else:\n",
    "        photos = keyframeDetection(path_to_vid, 0.31)\n",
    "        print(\"frames nr.: {}\".format(len(photos)))\n",
    "        black_point = 228\n",
    "        print(\"light video\")\n",
    "\n",
    "\n",
    "    print(\"detecting items\")\n",
    "    dicts = detect_multiple_images(photos, detection_graph)\n",
    "\n",
    "    # Now we have bounding boxes, we can get the (coordinates of)\n",
    "    # the parts of the frame that correspond to them\n",
    "    # We also keep the label of the box and the confidence level\n",
    "    print(\"detection successful\")\n",
    "    coordinates = []\n",
    "    for i, x in enumerate(dicts):\n",
    "        coord = get_abs_coord(photos[i], x, 0.99)\n",
    "        coordinates.append(coord)\n",
    "\n",
    "    # Count the occurences of each label to decide what kind of video it is\n",
    "    labels = [part[5] for coord in coordinates for part in coord]\n",
    "    d = {x: labels.count(x) for x in labels}\n",
    "\n",
    "    if len(d) < 2 or len(labels) < 200:\n",
    "        print(\"failed to detect sufficient info, exiting\")\n",
    "        continue\n",
    "\n",
    "    if 2 not in d or 1 not in d:\n",
    "        video_type = \"weekly\"\n",
    "    elif 3 not in d or 5 not in d:\n",
    "        video_type = \"daily\"\n",
    "    elif 7 not in d:\n",
    "        video_type = \"daily\"\n",
    "    else:\n",
    "        if (d[1] + d[2]) < (\n",
    "                d[4] + d[6]):\n",
    "            video_type = \"weekly\"\n",
    "        else:\n",
    "            video_type = \"daily\"\n",
    "\n",
    "    print(\"type of video: {}\".format(video_type))\n",
    "\n",
    "    # OCR\n",
    "    ## Weekly Videos first\n",
    "    if video_type == \"weekly\":\n",
    "        pickup_over = []\n",
    "        week_imgs = []\n",
    "        for coord in coordinates:\n",
    "            for part in coord:\n",
    "                # Looking for the weekly indicator (e.g. \"This week\")\n",
    "                if part[5] == 8:\n",
    "                    week_imgs.append(coord)\n",
    "                    break\n",
    "        # Initialize the lists\n",
    "        not_over = []\n",
    "        not_app = []\n",
    "        pick_over = []\n",
    "        pick_app = []\n",
    "        week_over = []\n",
    "        for i, photo in enumerate(photos):\n",
    "                # separate the detected boxes by label\n",
    "                for part in coordinates[i]:\n",
    "                    if part[5] == 3:\n",
    "                        img = photo[part[0]:part[1], part[2]:part[3]]\n",
    "                        not_over.append(img)\n",
    "                    elif part[5] == 4:\n",
    "                        img = photo[part[0]:part[1], part[2]:part[3]]\n",
    "                        not_app.append(img)\n",
    "                    elif part[5] == 5:\n",
    "                        img = photo[part[0]:part[1], part[2]:part[3]]\n",
    "                        pick_over.append(img)\n",
    "                    elif part[5] == 6:\n",
    "                        img = photo[part[0]:part[1], part[2]:part[3]]\n",
    "                        pick_app.append(img)\n",
    "                    elif part[5] == 7:\n",
    "                        img = photo[part[0]:part[1], part[2]:part[3]]\n",
    "                        week_over.append(img)\n",
    "                    else:\n",
    "                        continue\n",
    "        print(\"getting pick ups\")\n",
    "        # Since they are very repetitive, only use the first 100 occurences\n",
    "        ## Saves time when processing and sorting\n",
    "        pickups = process_week_app(pick_app[0:100])\n",
    "        print(\"getting notifications\")\n",
    "        notes = process_week_app(not_app[0:100])\n",
    "\n",
    "        notification = []\n",
    "        for image in not_over:\n",
    "            s = app_to_text(image, 0, 5, 0, video_type)\n",
    "            s = s.dropna()\n",
    "            if len(s) == 0:\n",
    "                continue\n",
    "            if s.iloc[-1, -2] > 80:\n",
    "                notification.append(s.iloc[-1, -1])\n",
    "        notification = list(set(notification))\n",
    "\n",
    "\n",
    "        for image in pick_over[0:15]:\n",
    "            s = app_to_text(image, 0, 5, 0, video_type)\n",
    "            s = s.dropna()\n",
    "            if stats.mean(s[\"conf\"]) > 95:\n",
    "                day = (s.iloc[-3,-1], s.iloc[-2,-1])\n",
    "                total = s.iloc[-1,-1]\n",
    "                pickup_over.append(day)\n",
    "                pickup_over.append(total)\n",
    "                break\n",
    "                \n",
    "        #Save the results\n",
    "        weekly_stats = {\"app_notes\": notes, \"pick_ups\": pickups,\n",
    "                        \"overall_notes\": notification, \"overall_pick_ups\": pickup_over}\n",
    "\n",
    "        outputs[direct] = weekly_stats\n",
    "        print (index)\n",
    "        print(\"---------------\")\n",
    "        \n",
    "\n",
    "    else:\n",
    "\n",
    "        # Common OCR mistakes and their fix\n",
    "        replacements = [(\"-—\", \"-\"), (\"—-\", \"-\"), (\"--\", \"-\"), (\"—\", \"-\"),\n",
    "                        (\"{\", \"(\"), (\"}\", \")\"), (\";\", \":\")]\n",
    "\n",
    "        digit_repl = [(\"I\", \"1\"), (\"i\", \"1\"), (\"e\", \"3\")]\n",
    "\n",
    "        app_replacements = [(\"Im \", \"1m \"), (\"im \", \"1m\"),\n",
    "                            (\"-—\", \"-\"), (\"—-\", \"-\"), (\"--\", \"-\"), (\"—\", \"-\")]\n",
    "\n",
    "        ## Get the apps into hourly lists\n",
    "\n",
    "        print(\"sorting the apps by hour\")\n",
    "        sorted_dicts = photos_to_dicts(photos, coordinates)\n",
    "\n",
    "        ## App Processing\n",
    "\n",
    "        apps = {}\n",
    "        for hour in sorted_dicts:\n",
    "            this_hour = apps_this_hour(hour)\n",
    "            apps[hour[0][1]] = this_hour\n",
    "\n",
    "        ## Dataframe Output\n",
    "        print(\"making a data frame\")\n",
    "        time_list = []\n",
    "        app_list = []\n",
    "        screen_list = []\n",
    "        backg_list = []\n",
    "        for key, values in apps.items():\n",
    "            for value in values:\n",
    "                time_list.append(key)\n",
    "                app_list.append(value[0][0])\n",
    "                if len(value[-1]) > 1:\n",
    "                    screen_list.append(value[-1][0])\n",
    "                    backg_list.append(value[-1][1])\n",
    "                else:\n",
    "                    screen_list.append(value[-1][0])\n",
    "                    backg_list.append(\"NA\")\n",
    "\n",
    "        ss = pd.DataFrame({\"time\": time_list, \"app\": app_list,\n",
    "                           \"screen\": screen_list, \"backg\": backg_list})\n",
    "        ss = ss[['time', 'app', 'screen', 'backg']]\n",
    "\n",
    "        outputs[direct] = ss\n",
    "        print (index)\n",
    "        print(\"---------------\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
